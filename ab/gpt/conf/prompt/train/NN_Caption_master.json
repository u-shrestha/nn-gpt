{
  "captioning_master": {
    "comment": [
      "Master prompt: emits a runnable captioning model with strict API. The LLM chooses decoder type (Transformer, LSTM, GRU) and hidden sizes/heads based on the classification inspirations and encoder.",
      "Ensures the model is trained using teacher forcing, returns proper shapes, and avoids undefined variables or untested custom attention modules."
    ],
    "input_list": [
      {"para": "nn_code", "value": "nn_code"}
    ],
    "addon_list": [
      {"para": "addon_nn_code_1", "value": "nn_code"},
      {"para": "addon_nn_code_2", "value": "nn_code"},
      {"para": "addon_nn_code_3", "value": "nn_code"},
      {"para": "addon_nn_code_4", "value": "nn_code"},
      {"para": "addon_nn_code_5", "value": "nn_code"},
      {"para": "addon_nn_code_6", "value": "nn_code"},
      {"para": "addon_nn_code_7", "value": "nn_code"},
      {"para": "addon_nn_code_8", "value": "nn_code"},
      {"para": "addon_nn_code_9", "value": "nn_code"},
      {"para": "addon_nn_code_10", "value": "nn_code"}
    ],
    "task": "img-captioning",
    "addon_task": "img-classification",
    "prompt": [
      "SYSTEM: Output exactly one fenced Python code block and nothing else.  Do not include prose or multiple code blocks.",
      "",
      "GOAL: Your task is to generate a high-performance image captioning model by taking inspiration from classification model code blocks, and by making safe, meaningful structural tweaks to the target captioning model.",
      "Do NOT output a classifier.  Produce exactly one full, runnable Python file that conforms to the API below.",
      "The emitted Python file must compile without syntax errors. Ensure all parentheses and brackets are balanced and all nn.Linear layers specify both input and output dimensions.",
      "We prioritize high BLEU on the COCO dataset while keeping the model simple, reliable, and easy to train.  Model size and speed are constraints.",
      "",
      "MANDATORY API (must pass validation):",
      "- def supported_hyperparameters(): return {{'lr','momentum'}}",
      "- class Net(nn.Module) with methods:",
      "    - __init__(self, in_shape: tuple, out_shape: tuple, prm: dict, device: torch.device)",
      "    - train_setup(self, prm)",
      "    - learn(self, train_data)",
      "    - forward(self, images, captions=None, hidden_state=None) -> (logits, hidden_state).",
      "- Expose the decoder as self.rnn and implement:",
      "    init_zero_hidden(batch: int, device: torch.device) -> (h0, c0).  For Transformer decoders return two empty tensors for compatibility.",
      "    forward(inputs, hidden_state, features=None) -> (logits, hidden_state).",
      "",
      "REUSE THE ENCODER BACKBONE:",
      "- Remove the classification head from the chosen classification blocks.  Keep the convolutional backbone for the encoder.  Produce a feature tensor [B,1,H] via AdaptiveAvgPool2d and a Linear layer.  Hidden dimension H must be ≥640 (e.g. 640 or 768). You may instead return all patch tokens from a ViT-style encoder to provide the decoder with multiple memory tokens.",
      "",
      "DECODER CHOICE:",
      "- YOU decide which decoder architecture best suits your chosen encoder and classification inspiration: use either nn.LSTM/GRU or nn.TransformerDecoder(batch_first=True).",
      "- Use an LSTM or GRU with an embedding layer and hidden size ≥640 (e.g. 640 or 768).  Initialise hidden state from the encoder features or pass features at each step.  You may include additive or dot-product attention on the encoder feature.",
      "- OR for a 1-layer nn.TransformerDecoder (batch_first=True), only use nn.MultiheadAttention(embed_dim, num_heads, batch_first=True) (note the lowercase ‘h’); choose num_heads dividing hidden_size (e.g. hidden_size=768, num_heads=8 or 12). This layer takes a query, key and value tensor, and returns the attention output and weights. Don’t define your own Self‑Attention or MultiHeadAttention classes. Use a simple positional encoding (sinusoidal or learnable embedding).  Cross-attend the decoder to the encoder memory (multi-token memory is encouraged).",
      "- You MUST decide the best hidden size and number of heads yourself; do not expect these values in the prompt.",
      "",
      "TEACHER FORCING & SHAPES:",
      "- If captions.ndim==3, set captions=captions[:,0,:].",
      "- inputs = captions[:,:-1]; targets = captions[:,1:].",
      "- forward() must return (logits, hidden_state) where logits has shape [B,T-1,vocab_size].  The first element of the returned tuple MUST be a Tensor.",
      "- Shape Sentry: assert images.dim()==4; assert logits.shape[1]==inputs.shape[1]; assert logits.shape[-1]==vocab_size.",
      "- in_channels = int(in_shape[1]); vocab_size = int(out_shape[0]).",
      "- forward() must return a tuple, where the first element is a Tensor of shape [B, T-1, vocab_size] (for teacher forcing) and the second element is the hidden state. Do not wrap logits in another tuple or return more than two values.",
      "",
      "TRAINING SETUP:",
      "- In train_setup(): call self.to(self.device); set self.criterion = nn.CrossEntropyLoss(ignore_index=0, label_smoothing=0.1); use torch.optim.AdamW(self.parameters(), lr=max(float(prm.get('lr',1e-3)), 3e-4), betas=(min(max(float(prm.get('momentum',0.9)),0.7),0.99), 0.999)).",
      "- In learn(): implement a loop over train_data; call logits,_ = self.forward(images,captions,None); compute loss as CrossEntropyLoss on logits.reshape(-1,V) and targets.reshape(-1); zero grad, backward, clip grads, step optimizer.  No scheduler unless simple (e.g. ReduceLROnPlateau).",
      "",
      "SAFE EDITS ONLY:",
      "- Reuse the base captioning skeleton.  Only modify: encoder blocks (e.g. bottleneck 1x1-3x3-1x1, SE/CBAM), decoder type and hidden dims/head counts, and simple attention on encoder features.",
      "- Do NOT create complex custom cross-attention or positional encodings.  Use nn.MultiheadAttention and PositionalEncoding or a simple embedding for positions.",
      "- Do NOT import torchvision or pretrained weights.  Define all layers in this file.",
      "- Use only valid PyTorch APIs from torch, torch.nn, torch.nn.functional. Do not invent modules (e.g., nn.SelfAttention is invalid).",
      "- Do not create or use custom attention modules (e.g. nn.SelfAttention or nn.MultiHeadAttention). Use only nn.MultiheadAttention or stick to LSTM/GRU.",
      "- Avoid referencing undefined variables or copying classification variables that are irrelevant.",
      "- The file must compile: no missing parentheses/brackets, every nn.Linear has two size args, no trailing characters.",
      "",
      "DIVERSITY REQUIREMENTS:",
      "- You MUST make at least three structural changes relative to the original captioning model: change the encoder block type, choose a different decoder family, vary hidden dimensions (≥640), add SE/CBAM or depthwise blocks, etc.",
      "- Randomise your choices per run to encourage a wide variety of models.  But keep within the safe modifications above.",
      "",
      "BLEU-ORIENTED GUIDANCE:",
      "- Larger hidden sizes and multi-head attention (when using Transformer) generally improve BLEU.  Hidden sizes in {{640, 768}} and num_heads in {{8, 12}} are good starting points.",
      "- Adding Squeeze-and-Excitation (SE) or CBAM to the encoder can help while staying within budget.",
      "- Keep dropout modest (0.1–0.3) in the decoder.  Clip gradients at 3.0.",
      "- Keep parameter count around 2 million.  Do not add untested modules.",
      "",
      "INSPIRATION CLASSIFICATION MODELS:",
      "```python",
      "{addon_nn_code_1}",
      "```",
      "```python",
      "{addon_nn_code_2}",
      "```",
      "```python",
      "{addon_nn_code_3}",
      "```",
      "```python",
      "{addon_nn_code_4}",
      "```",
      "```python",
      "{addon_nn_code_5}",
      "```",
      "```python",
      "{addon_nn_code_6}",
      "```",
      "```python",
      "{addon_nn_code_7}",
      "```",
      "```python",
      "{addon_nn_code_8}",
      "```",
      "```python",
      "{addon_nn_code_9}",
      "```",
      "```python",
      "{addon_nn_code_10}",
      "```",
      "",
      "ORIGINAL CAPTIONING CODE (reference only):",
      "```python",
      "{nn_code}",
      "```",
      "",
      "WRITE THE FINAL RUNNABLE FILE NOW.  Do not include extra commentary."
    ]
  }
}
