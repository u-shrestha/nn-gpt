{
  "system_prompt_template": "\n**CRITICAL EXECUTION REQUIREMENTS** (Code must be directly executable by the evaluation tool):\n\n1. **MANDATORY: Start with imports**:\n```python\nimport torch\nimport torch.nn as nn\n```\n\n2. **MANDATORY: Define supported_hyperparameters() function OUTSIDE all classes**:\n```python\ndef supported_hyperparameters():\n    # DEFAULT: only learning rate and momentum\n    return {'lr', 'momentum'}\n```\n- This function is REQUIRED.\n- **NEVER** list hyperparameters that are not actually used in the code.\n- `weight_decay` and `dropout` are OPTIONAL hyperparameters:\n  - If (and only if) you use weight decay inside the optimizer (e.g. `weight_decay=prm.get('weight_decay', 0.0)`), then update this function to:\n    ```python\n    def supported_hyperparameters():\n        return {'lr', 'momentum', 'weight_decay'}\n    ```\n  - If (and only if) you use nn.Dropout **and** read a `dropout` value from prm (e.g. `p=prm.get('dropout', 0.3)`), then update this function to:\n    ```python\n    def supported_hyperparameters():\n        return {'lr', 'momentum', 'dropout'}\n    ```\n  - If you use **both**, then:\n    ```python\n    def supported_hyperparameters():\n        return {'lr', 'momentum', 'weight_decay', 'dropout'}\n    ```\n\n3. **CRITICAL: Helper blocks (if any) MUST come BEFORE Net class**:\n   - Define ALL helper classes (e.g. Block, CustomLayer, etc.) BEFORE the Net class.\n   - Any helper used inside Net.__init__ MUST be defined above the Net class.\n\n4. **Net class MUST include ALL these methods INSIDE the class**:\n   - `__init__(self, in_shape, out_shape, prm, device)` – REQUIRED\n   - `forward(self, x)` – REQUIRED\n   - `train_setup(self, prm)` – REQUIRED\n   - `learn(self, train_data)` – REQUIRED\n\n5. **Complete, executable code only** – No placeholders, no `...`, no comments like `# todo` instead of real code.\n\n**STRUCTURE TEMPLATE** (FOLLOW THIS ORDER):\n```python\nimport torch\nimport torch.nn as nn\n\ndef supported_hyperparameters():\n    # Only list the hyperparameters you really use in the code\n    return {'lr', 'momentum'}\n\n# Optional: helper blocks BEFORE Net\nclass Block(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        # ... (complete implementation)\n    def forward(self, x):\n        # ...\n        return x\n\nclass Net(nn.Module):\n    def __init__(self, in_shape, out_shape, prm, device):\n        super().__init__()\n        self.device = device\n        # Initialize layers here\n        # Use prm.get('dropout', 0.0) or prm.get('weight_decay', 0.0) ONLY if you also list them in supported_hyperparameters\n\n    def forward(self, x):\n        # Forward pass\n        return x\n\n    def train_setup(self, prm):\n        # Move to device and create optimizer/criterion\n        self.to(self.device)\n        self.criteria = (nn.CrossEntropyLoss().to(self.device),)\n        self.optimizer = torch.optim.SGD(\n            self.parameters(),\n            lr=prm.get('lr', 0.01),\n            momentum=prm.get('momentum', 0.9),\n        )\n\n    def learn(self, train_data):\n        self.train()\n        for inputs, labels in train_data:\n            inputs = inputs.to(self.device)\n            labels = labels.to(self.device)\n            self.optimizer.zero_grad()\n            outputs = self(inputs)\n            loss = self.criteria[0](outputs, labels)\n            loss.backward()\n            self.optimizer.step()\n```\n\n**HYPERPARAMETER RULES (VERY IMPORTANT):**\n- Always start with `{'lr', 'momentum'}`.\n- Only add `weight_decay` to supported_hyperparameters **if you actually use it in the optimizer**.\n- Only add `dropout` to supported_hyperparameters **if you actually use prm['dropout'] to configure nn.Dropout**.\n- Do **NOT** list `weight_decay` or `dropout` when they are not used anywhere in the model.\n",
  "prefix_code": "```python\nimport torch\nimport torch.nn as nn\n\ndef supported_hyperparameters():\n    return {'lr', 'momentum'}\n\n"
}
