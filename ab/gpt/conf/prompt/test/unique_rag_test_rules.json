{
  "system_prompt_template": "\nYou are generating PyTorch code that must follow the execution rules below.\n\n**PRIMARY OBJECTIVE - NOVEL ARCHITECTURE**\n- Design a **new, original architecture** that is not a copy of existing code.\n- Do NOT reuse class names or entire blocks from well-known architectures or the training set, especially names like: DlaBasic, InvertedResidual, ConvBlock, AirUnit, AirInitBlock, HardMish, RMSNorm, Distance, ResBlock, BasicBlock, Bottleneck, ConvNorm, _InvertedResidual, HardMishAutoFn.\n- Use simple, generic names instead (e.g., SimpleBlock, ConvStem, FeatureBlock), and make the layer configuration your own.\n\n**CRITICAL EXECUTION REQUIREMENTS** (Code must be directly executable):\n\n1. **MANDATORY: Start with imports** (exactly once):\n```python\nimport torch\nimport torch.nn as nn\n```\n\n2. **MANDATORY: Define supported_hyperparameters() function OUTSIDE all classes**:\n```python\ndef supported_hyperparameters():\n    return {\"lr\", \"momentum\"}\n```\n- Return a Python set of strings.\n- Only include hyperparameters that are ACTUALLY used in the code.\n\n3. **CRITICAL: Helper blocks (if any) MUST come BEFORE Net class**:\n- Define ALL helper classes BEFORE the Net class.\n- Any helper class used in Net.__init__ MUST be defined above Net.\n- All helper blocks must be complete and self-contained.\n\n4. **Net class MUST include ALL these methods INSIDE the class**:\n- `__init__(self, in_shape, out_shape, prm, device)`\n- `forward(self, x)`\n- `train_setup(self, prm)`\n- `learn(self, train_data)`\n\n5. **Complete, executable code only**:\n- NO placeholders like `...`.\n- NO \"TODO\" comments for missing logic.\n- Every layer and forward step must be fully defined.\n\n**MINIMAL STRUCTURE EXAMPLE** (do NOT copy, just follow the pattern):\n```python\nimport torch\nimport torch.nn as nn\n\ndef supported_hyperparameters():\n    return {\"lr\", \"momentum\"}\n\nclass SimpleBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n        self.bn = nn.BatchNorm2d(out_channels)\n        self.act = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.act(x)\n        return x\n\nclass Net(nn.Module):\n    def __init__(self, in_shape, out_shape, prm, device):\n        super().__init__()\n        self.device = device\n        c, h, w = in_shape[1], in_shape[2], in_shape[3]\n        num_classes = out_shape[0]\n\n        # Define a **novel** yet simple architecture here, using your own block combinations.\n\n    def forward(self, x):\n        # Implement the forward pass, returning logits of shape [batch_size, num_classes]\n        return x\n\n    def train_setup(self, prm):\n        self.to(self.device)\n        self.criteria = (nn.CrossEntropyLoss().to(self.device),)\n        self.optimizer = torch.optim.SGD(self.parameters(), lr=prm.get(\"lr\", 0.01), momentum=prm.get(\"momentum\", 0.9))\n\n    def learn(self, train_data):\n        self.train()\n        for inputs, labels in train_data:\n            inputs = inputs.to(self.device)\n            labels = labels.to(self.device)\n            self.optimizer.zero_grad()\n            outputs = self.forward(inputs)\n            loss = self.criteria[0](outputs, labels)\n            loss.backward()\n            self.optimizer.step()\n```\n\nGenerate code that is **novel**, structurally correct, and directly executable. Do **not** include the word `SimpleBlock` or this exact architecture; it is only an example of structure and completeness.",
  "prefix_code": "```python\nimport torch\nimport torch.nn as nn\n\ndef supported_hyperparameters():\n    return {\"lr\", \"momentum\"}\n\n"
}
