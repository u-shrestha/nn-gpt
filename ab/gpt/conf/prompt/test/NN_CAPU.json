{
  "captioning_master": {
    "comment": [
      "Master prompt: emits a runnable captioning model with strict API. The LLM chooses decoder type (Transformer, LSTM, GRU) and hidden sizes/heads based on the classification inspirations and encoder.",
      "Ensures the model is trained using teacher forcing, returns proper shapes, and avoids undefined variables or untested custom attention modules.",
      "Uses a fixed skeleton so that the final model always implements the required API. The LLM may only modify the marked TODO sections in the skeleton."
    ],
    "input_list": [
      {"para": "nn_code", "value": "nn_code"}
    ],
    "addon_list": [
      {"para": "addon_nn_code_1", "value": "nn_code"},
      {"para": "addon_nn_code_2", "value": "nn_code"},
      {"para": "addon_nn_code_3", "value": "nn_code"},
      {"para": "addon_nn_code_4", "value": "nn_code"},
      {"para": "addon_nn_code_5", "value": "nn_code"}
    ],
    "task": "img-captioning",
    "addon_task": "img-classification",
    "prompt": [
      "SYSTEM: Output exactly one fenced Python code block and nothing else. Do not include any commentary or multiple code blocks.",
      "",
      "GOAL: Generate a high-performance, runnable image captioning model inspired by classification model code blocks. The model must reuse the encoder backbone (removing classification heads) to produce memory features [B, S, H] with hidden dimension H≥640, suitable for decoding. The decoder architecture is your choice between nn.LSTM, nn.GRU, or nn.TransformerDecoder (batch_first=True), with hidden sizes and number of heads chosen to match or exceed H. Use standard nn.MultiheadAttention if Transformer is selected; do not define custom attention modules. The model must be trainable using teacher forcing and produce outputs with correct shapes. Model size and speed constraints apply.",
      "",
      "ENCODER: Replace the classifier block with a feature extractor producing memory tensor [B, S, H], where H≥640 (e.g., 640 or 768). You may pool to a single vector or output multitudes of patch tokens (ViT-style). Adding SE/CBAM or bottleneck blocks is allowed as structural tweaks.",
      "",
      "DECODER: Choose one decoder : LSTM or GRU with embeddings and hidden size≥640, conditioning hidden states on encoder features, OR Transformer decoder built with nn.TransformerDecoderLayer and nn.TransformerDecoder (batch_first=True), with num_heads dividing hidden size (e.g., hidden_size=768, num_heads=8 or 12). Cross-attend the decoder queries to encoder memory tokens using nn.MultiheadAttention, no custom or untested modules. Strictly implement teacher forcing: if captions.ndim==3, flatten first dimension; inputs = captions[:,:-1], targets = captions[:,1:]. Forward must return (logits, hidden_state), where logits shape = [B, T-1, vocab_size].",
      "",
      "API SKELETON: The following skeleton defines the mandatory API functions and method structure. You MUST build your model by modifying only the sections marked TODO. Do not remove or rename any functions or variables outside the TODO sections. The resulting file must compile and adhere to the API requirements.",
      "```",
      "import math",
      "import torch",
      "import torch.nn as nn",
      "import torch.nn.functional as F",
      "from typing import Optional",
      "",
      "def supported_hyperparameters():",
      "    return {{'lr','momentum'}}",
      "",
      "class Net(nn.Module):",
      "    def __init__(self, in_shape: tuple, out_shape: tuple, prm: dict, device: torch.device):",
      "        super().__init__()",
      "        self.device = device",
      "        self.vocab_size = int(out_shape)",
      "        in_channels = int(in_shape[1])",
      "        self.hidden_dim = 640",
      "        # TODO: Replace self.encoder with custom encoder producing memory tensor [B, S, H] where H >= 640",
      "        self.encoder = nn.Identity()",
      "        # TODO: Replace self.rnn with custom decoder implementing forward(inputs, hidden_state, features) -> (logits, hidden_state)",
      "        self.rnn = nn.Identity()",
      "        self.criterion = nn.CrossEntropyLoss(ignore_index=0, label_smoothing=0.1)",
      "        self.optimizer = None",
      "",
      "    def init_zero_hidden(self, batch: int, device: torch.device):",
      "        return torch.empty(0, device=device), torch.empty(0, device=device)",
      "",
      "    def train_setup(self, prm: dict):",
      "        self.to(self.device)",
      "        lr = max(float(prm.get('lr', 1e-3)), 3e-4)",
      "        beta1 = min(max(float(prm.get('momentum', 0.9)), 0.7), 0.99)",
      "        self.optimizer = torch.optim.AdamW(self.parameters(), lr=lr, betas=(beta1, 0.999))",
      "        self.criterion = self.criterion.to(self.device)",
      "",
      "    def learn(self, train_data):",
      "        self.train()",
      "        for images, captions in train_data:",
      "            images = images.to(self.device, dtype=torch.float32)",
      "            caps = captions[:,0,:].long().to(self.device) if captions.ndim == 3 else captions.long().to(self.device)",
      "            inputs = caps[:, :-1]",
      "            targets = caps[:, 1:]",
      "            memory = self.encoder(images)",
      "            logits, _ = self.rnn(inputs, None, memory)",
      "            assert images.dim() == 4",
      "            assert logits.shape == inputs.shape[1]",
      "            assert logits.shape[-1] == self.vocab_size",
      "            loss = self.criterion(logits.reshape(-1, self.vocab_size), targets.reshape(-1))",
      "            self.optimizer.zero_grad(set_to_none=True)",
      "            loss.backward()",
      "            torch.nn.utils.clip_grad_norm_(self.parameters(), 3.0)",
      "            self.optimizer.step()",
      "",
      "    def forward(self, images: torch.Tensor, captions: Optional[torch.Tensor] = None, hidden_state=None):",
      "        images = images.to(self.device, dtype=torch.float32)",
      "        memory = self.encoder(images)",
      "        if captions is not None:",
      "            caps = captions[:,0,:].long().to(self.device) if captions.ndim == 3 else captions.long().to(self.device)",
      "            inputs = caps[:, :-1]",
      "            logits, hidden_state = self.rnn(inputs, hidden_state, memory)",
      "            assert logits.shape == inputs.shape[1]",
      "            assert logits.shape[-1] == self.vocab_size",
      "            return logits, hidden_state",
      "        else:",
      "            raise NotImplementedError()",
      "```",
      "",
      "Strictly follow the above API skeleton; modify only the sections labeled TODO. Do not change function signatures or add new public methods. Ensure the file compiles without syntax errors. Do not import torchvision or pretrained weights. Use only torch/torch.nn and torch.nn.functional APIs. No custom attention modules; only standard nn.MultiheadAttention permitted. Use teacher forcing exactly as described. Train using AdamW with learning rate and momentum from prm, clipping gradients at 3.0.",
      "",
      "INSPIRATION CLASSIFICATION MODELS:",
      "```",
      "{addon_nn_code_1}",
      "```",
      "```",
      "{addon_nn_code_2}",
      "```",
      "```",
      "{addon_nn_code_3}",
      "```",
      "```",
      "{addon_nn_code_4}",
      "```",
      "```",
      "{addon_nn_code_5}",
      "```",
      "",
      "ORIGINAL CAPTIONING CODE (reference only):",
      "```",
      "{nn_code}",
      "```",
      "",
      "WRITE THE FINAL RUNNABLE FILE NOW. Do not include extra commentary."
    ]
  }
}