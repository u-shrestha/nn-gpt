import torch
import torch.nn as nn
import numpy as np
import os
import gc
import traceback
import torchvision
from torch.nn import MaxPool2d
import torch.utils.checkpoint as cp

# -------------------------------------------------
# Wrapper Class (Included directly)
# -------------------------------------------------
class TorchVision(nn.Module):
    def __init__(self, model: str, weights: str = "DEFAULT", unwrap: bool = True, truncate: int = 2, split: bool = False):
        super().__init__()
        # Robust loading
        if hasattr(torchvision.models, "get_model"):
            self.m = torchvision.models.get_model(model, weights=weights)
        else:
            self.m = torchvision.models.__dict__[model](pretrained=bool(weights))

        if unwrap:
            layers = list(self.m.children())
            if isinstance(layers[0], nn.Sequential):
                layers = [*list(layers[0].children()), *layers[1:]]
            # Truncate head
            self.m = nn.Sequential(*(layers[:-truncate] if truncate else layers))
            self.split = split
        else:
            self.split = False
            self.m.head = self.m.heads = nn.Identity()

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        if self.split:
            y = [x]
            y.extend(m(y[-1]) for m in self.m)
            return y
        return self.m(x)

# -------------------------------------------------
# Helper: Handle 2D/3D/4D outputs
# -------------------------------------------------
def adaptive_pool_flatten(x):
    if x.ndim == 4: # CNN: (B, C, H, W)
        return torch.nn.functional.adaptive_avg_pool2d(x, (1, 1)).flatten(1)
    elif x.ndim == 3: # ViT/Swin: (B, L, C)
        return x.mean(dim=1)
    return x # Already 2D

# -------------------------------------------------
# AMP / Hyperparams
# -------------------------------------------------
from torch.amp import autocast, GradScaler
def autocast_ctx(enabled=True):
    return autocast("cuda", enabled=enabled)
def make_scaler(enabled=True):
    return GradScaler("cuda", enabled=enabled)

def supported_hyperparameters():
    return { 'lr', 'dropout', 'momentum' }

# -------------------------------------------------
# Generated Conv Block (Base Stream)
# -------------------------------------------------
def drop_conv3x3_block(in_channels, out_channels, stride=1, padding=1, bias=False, dropout_prob=0.0):
    # This sequence is generated by your script
    return nn.Sequential(
    $$
    )

# -------------------------------------------------
# Fractal Components
# -------------------------------------------------
class FractalBlock(nn.Module):
    def __init__(self, in_channels, out_channels, num_columns, loc_drop_prob, dropout_prob):
        super().__init__()
        self.num_columns = int(num_columns)
        self.loc_drop_prob = float(loc_drop_prob)
        depth = 2 ** max(self.num_columns - 1, 0)
        blocks = []
        for i in range(depth):
            level = nn.Sequential()
            for j in range(self.num_columns):
                if (i + 1) % (2 ** j) == 0:
                    in_ch_ij = in_channels if (i + 1 == 2 ** j) else out_channels

                    level.add_module(
                        f"subblock{j + 1}",
                        nn.Conv2d(in_ch_ij, out_channels, kernel_size=3, stride=1, padding=1, bias=False)
                    )

            blocks.append(level)
        self.blocks = nn.Sequential(*blocks)
        self.use_checkpoint_per_subblock = False

    def forward(self, x):
        outs = [x] * self.num_columns
        for level_block in self.blocks:
            if self.use_checkpoint_per_subblock:
                outs_i = [cp.checkpoint(blk, inp, use_reentrant=False) for blk, inp in zip(level_block, outs)]
            else:
                outs_i = [blk(inp) for blk, inp in zip(level_block, outs)]
            joined = torch.stack(outs_i, dim=0).mean(dim=0)
            outs[:len(level_block)] = [joined] * len(level_block)
        return outs[0]


class FractalUnit(nn.Module):
    def __init__(self, in_channels, out_channels, num_columns, loc_drop_prob, dropout_prob):
        super().__init__()
        self.block = FractalBlock(in_channels, out_channels, num_columns, loc_drop_prob, dropout_prob)
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
        self.use_checkpoint_whole = False

    def forward(self, x):
        if self.use_checkpoint_whole:
            x = cp.checkpoint(self.block, x, use_reentrant=False)
        else:
            x = self.block(x)
        return self.pool(x)


# -------------------------------------------------
# Net (Fusion Architecture)
# -------------------------------------------------
class Net(nn.Module):
    param_count_threshold: int = 80_000_000

    def __init__(self, in_shape: tuple, out_shape: tuple, prm: dict, device: torch.device) -> None:
        super().__init__()
        self.device = device
        self.glob_drop_ratio = 0.5
        self.loc_drop_prob = 0.15
        self.use_amp = False
        self.use_checkpoint = False
        self.param_count_threshold = int(prm.get("param_count_threshold", self.param_count_threshold))

        # --- Filled by Generator ---
        N = ?1
        num_columns = ?2
        element_list = ?? # Just for logging
        backbone_a_name = ?bb_a
        backbone_b_name = ?bb_b

        dropout_prob = float(prm['dropout'])

        # 1. Build Base Stream (Fractal CNN)
        self.build_fractal_stream(
            N=int(N),
            num_columns=int(num_columns),
            dropout_prob=dropout_prob,
            in_shape=in_shape
        )

        # 2. Build Backbone A
        self.backbone_a = TorchVision(backbone_a_name, "DEFAULT", True, 1).to(device)

        # 3. Build Backbone B
        self.backbone_b = TorchVision(backbone_b_name, "DEFAULT", True, 1).to(device)

        # 4. Dimension Inference (Dummy Pass)
        self.infer_dimensions(in_shape, out_shape[0])

        # Optimize setup
        param_count = sum(p.numel() for p in self.parameters() if p.requires_grad)
        if param_count > self.param_count_threshold:
            self.use_amp = True
            self.use_checkpoint = True

        # Propagate checkpoint flags
        for m in self.modules():
            if isinstance(m, FractalUnit): m.use_checkpoint_whole = self.use_checkpoint
            if isinstance(m, FractalBlock): m.use_checkpoint_per_subblock = self.use_checkpoint

        self._scaler = make_scaler(enabled=self.use_amp)

    # ---------- builder helpers ----------
    def _parse_in_shape(self, in_shape):
        if len(in_shape) == 4: _, C, H, W = in_shape
        elif len(in_shape) == 3: C, H, W = in_shape
        else: raise ValueError(f"in_shape must be length 3 or 4, got {in_shape}")
        return int(C), int(H), int(W)

    def build_fractal_stream(self, N, num_columns, dropout_prob, in_shape):
        C, H, W = self._parse_in_shape(in_shape)
        # Simple channel growth strategy
        channels = [64 * (2 ** (i if i != 4 else i - 1)) for i in range(N)]
        dropout_probs = [min(0.5, dropout_prob + 0.1 * i) for i in range(N)]

        self.features = nn.Sequential()
        in_channels = C
        for i, out_channels in enumerate(channels):
            unit = FractalUnit(
                in_channels=in_channels,
                out_channels=out_channels,
                num_columns=num_columns,
                loc_drop_prob=self.loc_drop_prob,
                dropout_prob=dropout_probs[i]
            )
            self.features.add_module(f"unit{i + 1}", unit)
            in_channels = out_channels

    def infer_dimensions(self, in_shape, num_classes):
        C, H, W = self._parse_in_shape(in_shape)
        with torch.no_grad():
            dummy = torch.zeros(1, C, H, W, dtype=torch.float32, device=self.device)

            # Base Stream
            # Temporary move to device for shape inference
            self.features.to(self.device)
            f_base = self.features(dummy)
            dim_base = f_base.view(1, -1).shape[1]

            # Backbone A
            f_a = self.backbone_a(dummy)
            f_a = adaptive_pool_flatten(f_a)
            dim_a = f_a.shape[1]

            # Backbone B
            f_b = self.backbone_b(dummy)
            f_b = adaptive_pool_flatten(f_b)
            dim_b = f_b.shape[1]

        # Fused Classifier
        self.output = nn.Linear(dim_base + dim_a + dim_b, num_classes)
        self._init_params()

    def _init_params(self):
        for m in self.features.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_uniform_(m.weight, a=np.sqrt(5))
                if m.bias is not None: nn.init.zeros_(m.bias)

    @staticmethod
    def _norm4d(x: torch.Tensor) -> torch.Tensor:
        if x.dim() == 4: return x
        if x.dim() == 5:
            B, T, C, H, W = x.shape
            return x.reshape(B * T, C, H, W)
        raise ValueError(f"Expected 4D/5D input, got {tuple(x.shape)}")

    # ---------- forward ----------
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        if next(self.parameters()).device != x.device:
            self.to(x.device)
        x = x.to(torch.float32)
        x = self._norm4d(x)

        # 1. Base Stream
        x_base = self.features(x)
        x_base = torch.flatten(x_base, start_dim=1)

        # 2. Backbone A
        x_a = self.backbone_a(x)
        x_a = adaptive_pool_flatten(x_a)

        # 3. Backbone B
        x_b = self.backbone_b(x)
        x_b = adaptive_pool_flatten(x_b)

        # 4. Fusion
        fused = torch.cat([x_base, x_a, x_b], dim=1)

        return self.output(fused)

    # ---------- training setup (Standard) ----------
    def train_setup(self, prm):
        self.to(self.device)
        self.criterion = nn.CrossEntropyLoss().to(self.device)
        self.optimizer = torch.optim.SGD(
            self.parameters(),
            lr=prm['lr'],
            momentum=prm['momentum']
        )
        self._scaler = make_scaler(enabled=self.use_amp)

    def learn(self, train_data):
        self.train()
        scaler = self._scaler
        train_iter = iter(train_data)
        try:
            for batch_idx, (inputs, labels) in enumerate(train_iter):
                inputs = inputs.to(self.device).float()
                labels = labels.to(self.device)
                self.optimizer.zero_grad(set_to_none=True)

                with autocast_ctx(enabled=self.use_amp):
                    outputs = self(inputs)
                    loss = self.criterion(outputs, labels)

                if not torch.isfinite(loss):
                    continue

                if self.use_amp:
                    scaler.scale(loss).backward()
                    scaler.unscale_(self.optimizer)
                    nn.utils.clip_grad_norm_(self.parameters(), 3.0)
                    scaler.step(self.optimizer)
                    scaler.update()
                else:
                    loss.backward()
                    nn.utils.clip_grad_norm_(self.parameters(), 3.0)
                    self.optimizer.step()
        finally:
            if hasattr(train_iter, 'shutdown'):
                train_iter.shutdown()
            del train_iter
            gc.collect()