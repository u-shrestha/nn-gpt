{
  "accuracy": 0.0,
  "batch": 64,
  "duration": 0,
  "lr": 0.01,
  "momentum": 0.9,
  "transform": "CenterCrop_RandomCrop_Pad",
  "uid": "",
  "error": "Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/ushashrestha/Documents/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/ushashrestha/Documents/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/ushashrestha/Documents/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/ushashrestha/Documents/.venv/lib/python3.10/site-packages/torchvision/datasets/cifar.py\", line 119, in __getitem__\n    img = self.transform(img)\n  File \"/home/ushashrestha/Documents/.venv/lib/python3.10/site-packages/torchvision/transforms/transforms.py\", line 95, in __call__\n    img = t(img)\n  File \"/home/ushashrestha/Documents/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/ushashrestha/Documents/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/ushashrestha/Documents/.venv/lib/python3.10/site-packages/torchvision/transforms/transforms.py\", line 681, in forward\n    i, j, h, w = self.get_params(img, self.size)\n  File \"/home/ushashrestha/Documents/.venv/lib/python3.10/site-packages/torchvision/transforms/transforms.py\", line 640, in get_params\n    raise ValueError(f\"Required crop size {(th, tw)} is larger than input image size {(h, w)}\")\nValueError: Required crop size (29, 29) is larger than input image size (27, 27)\n"
}